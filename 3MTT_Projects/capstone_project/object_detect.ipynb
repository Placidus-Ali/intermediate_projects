{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fab9e86-9365-47f4-bdcd-2ee96527b264",
   "metadata": {},
   "source": [
    "### Real-Time Object Detection System\n",
    "\n",
    "This project uses the Pascal VOC dataset to develop a model that detects objects in live video feeds. You’ll begin by preparing a subset of the COCO dataset with classes relevant to your goals, such as \"person, car, bicycle, dog, cat or bus\" and perform data augmentation for robust training. With YOLOv5, a popular model for real-time applications, you’ll load pretrained weights and fine-tune them using your filtered dataset, aiming for accuracy and speed. \n",
    "\n",
    "For inference, you’ll integrate OpenCV to process video from a webcam or video file, running object detection in real time. The model draws bounding boxes around detected objects and labels them on the video feed, demonstrating strong applicability in real-time scenarios. \n",
    "\n",
    "Finally, you’ll deploy the solution using Streamlit, where users can upload video files or use their webcams to view real-time detections directly from a web interface. Streamlit’s simple deployment pipeline allows easy sharing of your project for remote access and feedback.\n",
    "\n",
    "With your background in data science and machine learning, this project leverages skills in model training, data preprocessing, and deployment, essential for your data science portfolio. It’s an excellent addition to your projects as it demonstrates practical applications of machine learning for real-time use cases—aligning with your goal to attract employers and stand out in the field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f028ef4d-6e1c-4ec0-990e-b41b7d9d124e",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f5102d-8b07-4107-baff-e9357a165277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\anaconda\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: numpy in c:\\anaconda\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.5.1 in c:\\anaconda\\lib\\site-packages (from torchvision) (2.5.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\anaconda\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\anaconda\\lib\\site-packages (from torch==2.5.1->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\anaconda\\lib\\site-packages (from torch==2.5.1->torchvision) (4.11.0)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda\\lib\\site-packages (from torch==2.5.1->torchvision) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\anaconda\\lib\\site-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\anaconda\\lib\\site-packages (from torch==2.5.1->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda\\lib\\site-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\anaconda\\lib\\site-packages (from torch==2.5.1->torchvision) (2024.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda\\lib\\site-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\anaconda\\lib\\site-packages (from jinja2->torch==2.5.1->torchvision) (2.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Anaconda\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Anaconda\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Anaconda\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7553ec47-767e-4afd-9279-61a0382df989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a453537c-1440-456b-a645-fa1819767412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(\"Torch and TorchVision are loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43c35d-b3a5-4fb5-8a5d-e3410da1e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdb596d-66e7-4c41-96ea-d3901ea0f9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e790dac8-5339-45e4-8b44-e48f522a6649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "print(torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b4b0e-96b5-49b0-8474-f6208bd67263",
   "metadata": {},
   "source": [
    "fasterrcnn_resnet50_fpn: Loads a pre-trained Faster R-CNN model with a ResNet50 backbone.\n",
    "model.eval(): Sets the model to evaluation mode.\n",
    "device: Automatically selects GPU if available, which will speed up the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f58e41e-ab54-4fac-a9c5-bc677ac2c55c",
   "metadata": {},
   "source": [
    "### Real-Time Object Detection with OpenCV\n",
    "We use OpenCV to capture a live video stream from the camera and perform object detection on each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "311afc07-2566-4995-bbd0-0d98522f0b85",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Transform to apply on each frame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      3\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[0;32m      4\u001b[0m ])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Start capturing video from the webcam\u001b[39;00m\n\u001b[0;32m      7\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "# Transform to apply on each frame\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Start capturing video from the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert frame to PIL image\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Apply transformation and pass to model\n",
    "    input_tensor = transform(pil_img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "    \n",
    "    # Draw bounding boxes and labels on the frame\n",
    "    for box, label, score in zip(outputs[0]['boxes'], outputs[0]['labels'], outputs[0]['scores']):\n",
    "        if score >= 0.5:  # Set threshold to 0.5 for detection confidence\n",
    "            box = box.to('cpu').numpy().astype(int)\n",
    "            cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label.item()} {score:.2f}\", (box[0], box[1] - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Real-Time Object Detection\", frame)\n",
    "\n",
    "    # Break loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af974c0b-01ca-4862-b7d5-0da2e7514130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452e195e-47e2-4d12-9629-44556232ad96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4751f5-617f-4dd8-94e1-b5028f5d65f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3d55b-37bf-4eb6-b034-fbd26f51e874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab6261e-f6cb-4403-872e-ba77c78f78d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
